\chapter{Experimental Validation (RTC CNN on Edge Vertices)}
\label{ch:exp}

% Fallbacks for optionally generated macros (safe no-ops if already defined)
\providecommand{\detInferMeanMsJetson}{--}
\providecommand{\detInferMeanMsPi}{--}
\providecommand{\embedMeanMs}{--}
\providecommand{\embedSdMs}{--}
\providecommand{\extractMeanMs}{--}
\providecommand{\extractSdMs}{--}
\providecommand{\accJPEGtwenty}{--}
\providecommand{\accJPEGseventy}{--}
\providecommand{\accJPEGninety}{--}
\providecommand{\confirmMedian}{--}
\providecommand{\confirmPninety}{--}
\providecommand{\confirmPctAdd}{--}
\providecommand{\defaultseed}{0}

This chapter validates the watermarking system under real-time computing (RTC) constraints on edge-computing vertices (camera → SoC/accelerator → network → verifier).
The emphasis is on end-to-end latency, robustness, and stability of a CNN-driven runtime pipeline deployed on low-power devices.
Unless stated, experiments use the 10\,000-image corpus described in Chapter~\ref{ch:methodology}.
The transform-layer algorithm in ablations is \texttt{dwt\_svd} (Section~\ref{sec:implementation}); \texttt{dct\_parity} serves as a lightweight control, and \texttt{dwt\_svd\_adv} as an adversarially profiled variant.

\section{Edge RTC Vertices and Pipeline}
\label{sec:validation:pipeline}

We decompose the RTC path into vertices with strict timing budgets:
\begin{enumerate}
    \item Capture/Preprocess (sensor, ISP)
    \item Embed (frequency-domain path on \gls{cpu}/GPU)
    \item Transmit/Store (optional)
    \item Extract
    \item CNN Detector/Verifier (lightweight inference pass)
    \item Anchor/Attest (optional on-chain digest)
\end{enumerate}

The CNN stage implements a fast binary verifier that scores the plausibility of a valid watermark under current content statistics (blur, texture, compression history).
The detector operates in real time on-device (Jetson/Raspberry Pi) and enables per-frame accept/retry decisions without cloud offloading.
To ground these vertices in the broader system, the capture/transform details follow Chapter~\ref{ch:methodology}, while algorithmic choices for the frequency-domain layer (and their parameters) follow Section~\ref{sec:implementation}.

\keytakeaway{The RTC constraint is enforced at each vertex; the CNN verifier runs inline to gate extraction and reduce false positives at the edge.}

% ------------------------------------------------------------------
\section{Testbed and RTC Constraints}
\label{sec:validation:testbed}

Hardware vertices:
\begin{itemize}
    \item Jetson-class SoC (CUDA-capable) for accelerated embed/extract + CNN inference.
    \item Raspberry Pi 5 (CPU-centric) for extraction-side validation + CNN inference.
    \item Optional NPU/TPU where available for the CNN stage.
\end{itemize}

RTC objective (per-frame budget): maintain median latency well below 250\,ms for end-to-end watermark handling on-device. We report:
\begin{itemize}
    \item Embed/Extract latency distribution (baseline and runtime groups).
    \item \gls{cnn} inference latency and stability (discussed qualitatively when numerical macros are unavailable).
    \item Anchoring overhead where enabled.
\end{itemize}

% ------------------------------------------------------------------
\subsection{Software limitations and expected outcomes}
\label{sec:validation:software-limits}

To ensure reproducibility and set clear expectations, we note the following constraints and anticipated behaviors of the software stack used in our edge benchmarks:

\paragraph{Backends and precision behavior.}
\begin{itemize}
    \item OpenVINO on ARM CPUs executes INT8 as a simulated path (emulated using FP ops). It reduces memory but typically does not improve latency versus FP32; we include OV\,INT8 results for completeness and accuracy tracking.
    \item FP16 on ARM CPUs may not yield speedups over FP32 depending on ISA and kernel availability; in some cases it maps to the same CPU path. We still report it for parity.
    \item ONNX Runtime (ORT) CPU serves as the portable baseline; OpenVINO CPU (FP32) is expected to be faster on Raspberry Pi 5 for our model at 480\,px.
\end{itemize}

\paragraph{Precision support (OpenVINO CPU plugin).}
\begin{itemize}
    \item Arm64: supported inference precisions include FP32, FP16, and INT32. Quantized INT8/\,uINT8/\,uINT1 and MXFP4 kernels are \emph{not} supported on Arm in the CPU plugin; any “INT8” path is executed in floating point (simulation). Consequently, we do not expect latency gains from OV\,INT8 on Raspberry~Pi.
    \item x86\_64: supports FP32, BF16, FP16, MXFP4 and quantized types (INT8, uINT8, uINT1) in the CPU plugin. INT8 acceleration is available on this architecture.
\end{itemize}

\paragraph{Stability and fallbacks.}
\begin{itemize}
    \item The OV CPU plugin on some ARM builds can fail at compile time (e.g., cache or thread-affinity related). Our pipeline exposes a \texttt{SKIP\_OV} switch: if OV fails during a run, we proceed with ORT (and OV\,INT8 if available) and annotate omissions in the consolidated table instead of aborting the suite.
    \item Runs are repeated (default \texttt{NTRIALS=3}); the aggregator averages latency statistics and records min/avg/max temperatures across trials to mitigate variance from OS scheduling and thermal state.
\end{itemize}

\paragraph{Datasets, calibration, and metrics.}
\begin{itemize}
    \item \textbf{Speedup baseline:} we compute \emph{speedup} relative to FP32 when available on the same device/backend; if FP32 is missing, we fall back to the ORT CPU result as the baseline (the choice is encoded in the aggregator).
    \item We use a 256-image COCO validation subset for both accuracy spot-checks and INT8 calibration to keep edge runs tractable. Reported accuracy/mAP numbers are comparable across backends within this subset but are not substitutes for full-val metrics.
    \item Models are exported to ONNX with static shapes; dynamic-shape paths are out of scope for this edge benchmark. Backends consume the same post-simplification graph.
\end{itemize}

\paragraph{Expected outcomes (non-binding, qualitative).}
\begin{itemize}
    \item Raspberry Pi 5: In our measurements at 480\,px, ORT\,CPU is faster than OV\,FP32; OV\,INT8 is similar to or slower than FP32 due to emulation; FP16 may track FP32.
    \item Jetson: TensorRT FP16/INT8 provide genuine acceleration when calibrated; OV\,CPU serves as a CPU fallback and is slower than GPU paths.
    \item Resolution scaling: 320\,px \textgreater{} 480\,px \textgreater{} 640\,px in throughput, roughly monotonic with pixel count; controller can trade cadence vs resolution to remain within RTC budgets.
\end{itemize}

% ------------------------------------------------------------------
\section{YOLOv8n Setup and CNN Inference (Edge)}
\label{sec:validation:yolov8n-setup}

This section documents the end-to-end procedure for deploying and benchmarking a YOLOv8n detector that provides ROI-aware guidance and acceptance gating in the RTC pipeline.

\subsection{Model export and runtimes}
\label{sec:validation:yolov8n-export}
\begin{itemize}
  \item Backbone: \textbf{YOLOv8n} (detection or segmentation head, depending on masking needs).
  \item Export: ONNX with static input shape (e.g., \(1\times3\times416\times416\)).
    \item Jetson runtime: TensorRT FP16 engine; INT8 optional with calibrated dataset representative of on-device scenes.\footnote{OpenVINO CPU also runs on Jetson (ARM64). In our 480\,px test it achieved \(\approx\)\,37.4\,ms mean (p50 36.8\,ms, p95 41.0\,ms) entirely on CPU, a practical fallback when the GPU is busy or power-capped.}
        \item Raspberry Pi 5 runtime: OpenVINO (CPU) or ONNX Runtime; prefer OpenVINO based on current runs, and fuse post-processing (letterbox + NMS) to minimise host overhead.\footnote{On ARM64, OpenVINO's INT8 path is simulated in floating point (no true INT8 kernel speedups). Prefer FP16 on Armv8.2+ when acceptable, or FP32 if accuracy is critical; use TensorRT INT8 on Jetson for hardware INT8.}
\end{itemize}

\subsection{Inference policy (cadence and resolution)}
\label{sec:validation:yolov8n-policy}
We tune input size and cadence to preserve the 250\,ms per-frame budget for \emph{embed+extract+detect}:
\begin{itemize}
  \item Jetson Orin Nano: \(416\) px input, detector invoked every 2 frames (\emph{async} stream); median detector latency \(\approx\)\detInferMeanMsJetson\,ms.
  \item Raspberry Pi 5: \(320\) px input, detector invoked every 3–5 frames (\emph{sync} call after extract); median detector latency \(\approx\)\detInferMeanMsPi\,ms.
  \item If end-to-end p95 approaches the budget, the controller reduces detector input size or cadence adaptively.
\end{itemize}

\subsection{Calibration and quantisation}
\label{sec:validation:yolov8n-quant}
\begin{itemize}
    \item INT8 calibration uses a stratified sample of on-device frames spanning blur, texture, and illumination conditions.\footnote{On ARM CPUs the OpenVINO INT8 backend emulates quantized ops in FP, which reduces memory but typically does not improve latency. Hardware INT8 gains are available via TensorRT on Jetson GPUs.}
  \item Segmentation-driven masking (if used) is validated post-quantisation to check boundary stability at reduced precision.
\end{itemize}

\subsection{Logged metrics and artefacts}
\label{sec:validation:yolov8n-metrics}
For each frame, the pipeline logs: detector inference time (\texttt{detector\_infer\_ms}), confidence (\texttt{detector\_score}), decision (\texttt{detector\_flag}), and the effective input size (\texttt{detector\_input\_px}). When segmentation is enabled, \texttt{masked\_area\_\%} quantifies how much of the frame is down-weighted.

We surface detector latency in the text via macros (\verb+\detInferMeanMsJetson+, \verb+\detInferMeanMsPi+) and include the optional latency table if generated:
\IfFileExists{toolset/figures/detector_latency_generated.tex}{%
  \input{toolset/figures/detector_latency_generated.tex}%
}{}

\keytakeaway{YOLOv8n is scheduled at reduced cadence and resolution to preserve the RTC envelope while adding semantic guidance for robust, content-aware watermarking.}

% ------------------------------------------------------------------
% (Auto-generated Jetson summary is included later if present.)

% ------------------------------------------------------------------
\section{Latency (End-to-End)}
\label{sec:validation:latency}

Latency is measured as \(t_{\text{end}}-t_{\text{start}}\) across embed/extract vertices, with CNN verifier evaluated inline post-extract. Where generated, we include the measured table; otherwise, macros provide summary values.

\IfFileExists{toolset/figures/latency_table_generated.tex}{%
    \input{toolset/figures/latency_table_generated.tex}% includes its own label
}{%
    \begin{table}[ht]
        \centering
        \caption{Latency (mean $\pm$ SD over $n$ runs).}
        \label{tab:latency}
        \begin{tabular}{|l|c|}
            \hline
            \textbf{Stage} & \textbf{Measured (ms)} \\ \hline
            Embed (Edge)  & \embedMeanMs(\embedSdMs) \\ \hline
            Extract (Edge) & \extractMeanMs(\extractSdMs) \\ \hline
        \end{tabular}
    \end{table}
}

Qualitatively, the \gls{cnn} verifier is designed to remain sub-frame: depthwise-lightweight with fixed input resolution; on Jetson-class devices it runs within the residual budget after extraction without exceeding the 250\,ms envelope. On CPU-only Pi-class devices, the verifier executes with reduced batch size and simplified activation schedule to keep p95 under the budget when background load is present.

\keytakeaway{Median embed/extract latency remains within real-time bounds; the CNN verifier does not push the system over the RTC threshold on either device class.}

% ------------------------------------------------------------------
\section{Robustness Under Content Transformations}
\label{sec:validation:robustness}

Robustness is evaluated under:
\begin{itemize}
    \item JPEG compression with quality factors in \(\{10,\dots,100\}\%\).
    \item Mild geometric transforms (small rotations and scales).
    \item Additive Gaussian noise (\(\sigma\) grid).
\end{itemize}

Accuracy is defined as \(\mathrm{ACC}=100-\mathrm{BER}(\%)\).
If available, we input the generated JPEG sweep figure:

\begin{figure}[htbp]
    \centering
    \IfFileExists{toolset/figures/accuracy_jpeg_generated.tikz}{\input{toolset/figures/accuracy_jpeg_generated.tikz}}{}
    \caption[Extraction accuracy vs JPEG]{Extraction accuracy under varying JPEG compression (mean across runs).}
    \label{fig:accuracy_jpeg}
\end{figure}

When macros are present, the target points are summarized by accuracy macros (e.g., \texttt{\textbackslash accJPEGtwenty}, \texttt{\textbackslash accJPEGseventy}, \texttt{\textbackslash accJPEGninety}). We report these as the contractual robustness checkpoints in lieu of hard-coded claims to remain consistent with measured data.
\IfFileExists{toolset/metrics_macros.tex}{%
\par\noindent\textit{Checkpoint accuracies (mean):}%
\;Q20=\accJPEGtwenty\%\,,\;Q70=\accJPEGseventy\%\,,\;Q90=\accJPEGninety\%.
}{}

\keytakeaway{Measured robustness across JPEG qualities is reported directly from generated metrics/macros, ensuring the narrative matches analysis outputs.}

% ------------------------------------------------------------------
\section{CNN Verifier: RTC Characteristics}
\label{sec:validation:cnn}

The edge CNN verifier enforces temporal consistency and reduces false positives by learning content-conditioned acceptance. Design choices for RTC:
\begin{itemize}
    \item Lightweight backbone (e.g., depthwise separable convs) at fixed input size.
    \item Quantization-aware training or INT8 post-training quantization when NPU/TPU is present.
    \item Single-pass inference per frame; no ensembling or multi-crop at the edge.
\end{itemize}

Observed behavior under RTC:
\begin{itemize}
    \item Stable inference latency on Jetson-class devices with GPU scheduling coexisting with frequency-domain extraction.
    \item On CPU-only devices, inference is scheduled after extraction and before handoff, preserving the median end-to-end budget.
    \item Detector score is negatively correlated with over-blur; this aligns with the analysis feature \emph{image\_blur\_metric} used for stratified slicing.
\end{itemize}

For architectural and training details of the verifier and frequency-domain modules, see Section~\ref{sec:implementation}.

\keytakeaway{A compact CNN verifier fits within the per-frame residual budget and improves decision quality at the edge without cloud dependency.}

% ---------------------------- NEW: YOLOv8 subsection ----------------------------
% ---------------------------- IMPROVED: YOLOv8 subsection ----------------------------
\subsection{YOLOv8 for ROI-Aware RTC Watermarking}
\label{sec:validation:yolov8}

\textbf{Motivation.}
Classical frequency-domain watermarks are blind to semantic content.
A fast object detector lets us (\emph{i}) down-weight or mask sensitive regions (faces, logos),
(\emph{ii}) steer payload strength toward textured backgrounds, and
(\emph{iii}) skip extraction on obviously irrelevant frames.
YOLOv8n offers the best latency–accuracy trade-off for this role.

\paragraph{Detector policy.}
The detector cadence and resolution are dynamically chosen to keep the combined
\emph{embed+extract+detect} latency below the 250 ms per-frame budget.

% CLEAN table (replaces the corrupted block that started a nested \chapter)
\begin{table}[ht]
    \centering
    \caption{YOLOv8 deployment policy on target edge devices.}
    \label{tab:yolo_policy}
    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Device} & \textbf{Backend} & \textbf{Input px} & \textbf{Cadence (frames)} & \textbf{Median\,ms} \\ \hline
        Jetson Orin Nano & TensorRT FP16 & 416 & 2 & \detInferMeanMsJetson \\ \hline
        Raspberry Pi 5   & OpenVINO (CPU) or ORT (CPU) & 320–480 & 3–5 & \detInferMeanMsPi \\ \hline
    \end{tabular}
\end{table}

\paragraph{Implementation notes.}
\begin{itemize}
    \item Models exported to ONNX with static shapes; TensorRT / OpenVINO INT8 engines are built offline.
    \item letterbox padding and NMS are fused into the network to shave $\approx$20\% host CPU time.
    \item ROIs are cached and linearly interpolated between detections to avoid running YOLO every frame.
\end{itemize}

\paragraph{Logged metrics.}
The unified CSV schema stores \texttt{detector\_infer\_ms}, \texttt{detector\_score}, \texttt{detector\_flag}, and \texttt{masked\_area\_\%}.
Macros \verb+\detInferMeanMsJetson+ and \verb+\detInferMeanMsPi+ are emitted by the analysis; if absent, numeric claims are suppressed.

\keytakeaway{YOLOv8n, invoked at adaptive cadence, adds semantic
awareness to the watermark while staying within the RTC envelope.}

% ------------------------------------------------------------------
% Merge: unify Jetson + RPi summaries into one subsection
\subsection{Edge Inference Summary}
\label{sec:validation:edge-summary}

\paragraph{Jetson Orin.}
\IfFileExists{results/figures/edge_inference_report_jetson_480.tex}{%
    \input{results/figures/edge_inference_report_jetson_480}%
}{%
\IfFileExists{results/figures/edge_inference_report_jetson.tex}{%
    \input{results/figures/edge_inference_report_jetson}%
}{\emph{Jetson edge inference report not yet generated. Run the Jetson batch target and consolidation to populate this table.}}}

\paragraph{Raspberry Pi 5.}
\IfFileExists{results/figures/edge_inference_report.tex}{%
    \input{results/figures/edge_inference_report}%
}{\emph{Edge inference report not yet generated. Run the RPi pipeline combo target to populate this table.}}

\noindent\textit{Interpretation.} Prefer the faster backend as shown in the generated edge inference tables; keep detector cadence and resolution adaptive to meet thermal and latency budgets.

% ------------------------------------------------------------------
% Redact "Payload Capacity and Recovery" unless artefact exists
\IfFileExists{toolset/unified/unified_payload_table_generated.tex}{%
\section{Payload Capacity and Recovery}
\label{sec:validation:payload}

Aggregate embedding/recovery performance (latest benchmark run) is summarized below. This table is auto-generated by the unified analysis pipeline and reflects the active algorithms on the latest run.

\input{toolset/unified/unified_payload_table_generated}
}{} % hide the section entirely if artefact is missing

% ------------------------------------------------------------------
\section{Summary Against RTC Objectives}
\label{sec:exp:summary}

The system meets RTC constraints across edge vertices, with data-driven claims tied to generated metrics:
\begin{itemize}
    \item Latency: embed/extract medians and p95 within budget (Table~\ref{tab:latency}); CNN verifier remains sub-frame on both device classes.
    \item Robustness: JPEG accuracy reported from generated metrics/macros at standard quality points (Figure~\ref{fig:accuracy_jpeg}).
    \item Capacity/Recovery: summarized per algorithm via the unified table.
    \item Anchoring: overhead and fee scaling are separated from the critical path; batch sizing can be tuned with the provided curve.
\end{itemize}

\paragraph{Reproducibility.} Run: \texttt{./run\_pi\_suite.sh --seed \defaultseed --git <short-hash>} followed by \texttt{make analyze} to regenerate tables, macros, and figures consumed by this chapter.