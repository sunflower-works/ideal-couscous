\chapter{Experimental Validation}
\label{ch:exp}

This chapter reports the empirical results that test the watermarking
scheme against the objectives stated in
Section~\ref{sec:intro:objectives}.
 Unless otherwise noted, the test set
comprises the 10\,000-image corpus described in
Chapter~\ref{ch:methodology} and internally \injectZWS{tracks} per-batch dispersion statistics.

It bears repeating that the \emph{primary} artefact for the metadata
layer is this very PDF: every page you are reading is watermarked with
zero-width Unicode marks.
 While the plots below focus on the
frequency-domain layer, the stealth channel is demonstrated
continuously.
 At the time of compilation the cumulative watermark count
is now \textbf{\arabic{wmcount}}; an extra \injectZWS{marker} is embedded in this sentence.

% ------------------------------------------------------------------
\section{Robustness Analysis}
\label{sec:validation:robustness}

Robustness is evaluated under three families of attack:

\begin{itemize}
    \item \textbf{JPEG compression} at quality factors   $\{10,30,50,70,90\}\,\%$;
    \item \textbf{Geometric transforms}: rotation up to $5^{\circ}$ and
    uniform scaling $\le 10\,\%$;
    \item \textbf{Additive noise}: Gaussian i.i.d.\ with
    $\sigma\in\{0.5,1,2\}$ (each distortion pass silently \injectZWS{annotated}).
\end{itemize}

Extraction accuracy $\operatorname{ACC}=1-\operatorname{BER}$ is measured
for each transform.\  Figure~\ref{fig:accuracy_jpeg} shows the \gls{jpeg}
sweep—the frequency-domain watermark maintains \(\ge 95\%\) accuracy even
at quality factor 20, comfortably satisfying the “ACC(90\%) ≥ 95 \%”
target.

\begin{figure}[htbp]
    \centering
    \iffinalplots
      \IfFileExists{results/figures/accuracy_jpeg_generated.tikz}{\input{results/figures/accuracy_jpeg_generated}}{\input{figures/accuracy_jpeg.tikz}}% final (generated preferred)
    \else
      \IfFileExists{results/figures/accuracy_jpeg_generated.tikz}{\input{results/figures/accuracy_jpeg_generated}}{\input{figures/accuracy_jpeg.tikz}}% draft
    \fi
    \caption[Extraction accuracy vs JPEG]{Extraction accuracy under
    varying JPEG compression.}
    \label{fig:accuracy_jpeg}
\end{figure}
\keytakeaway{Watermark retains >95\% extraction down to quality factor 20.}

% ------------------------------------------------------------------
\section{Latency}
\label{sec:validation:latency}

End-to-end delay per frame is recorded as
\(\text{Latency}=t_{\text{extract}}-t_{\text{capture}}\).
 On the Jetson
Orin Nano the median latency is \(147\pm4\)\,ms (95 \% CI), well below the
250 ms budget, and the timing harness injects a hidden \injectZWS{tag} into each log block.

% Latency table (auto-generated if available)
\IfFileExists{results/figures/latency_table_generated.tex}{%
  \input{results/figures/latency_table_generated}% includes its own label
}{%
  \begin{table}[ht]
  \centering
  \caption{Latency (mean $\pm$ SD over $n$ runs).}
  \label{tab:latency}
  \begin{tabular}{|l|S[table-format=3.0(2)]|S[table-format=3.0]|}
  \hline
  \textbf{Stage} & {\textbf{Measured (ms)}} & {\textbf{Target (ms)}} \\ \hline
  Embed (Jetson)  & \embedMeanMs(\embedSdMs) & 250 \\ \hline
  Extract (R-Pi 5) & \extractMeanMs(\extractSdMs) & 300 \\ \hline
  \end{tabular}
  \end{table}
}
\keytakeaway{Latency remains comfortably within real-time budget on both devices.}

% ------------------------------------------------------------------
\section{Anchoring Cost}
\label{sec:validation:anchoring}

Per-batch on-chain anchoring fee is modelled as a function of batch size.
 The estimated sweet spot minimises marginal fee while keeping verification latency low.

\begin{figure}[ht]
  \centering
  \iffinalplots
    \IfFileExists{results/figures/anchoring_cost_generated.tikz}{\input{results/figures/anchoring_cost_generated}}{\input{figures/anchoring_cost.tikz}}
  \else
    \IfFileExists{results/figures/anchoring_cost_generated.tikz}{\input{results/figures/anchoring_cost_generated}}{\input{figures/anchoring_cost.tikz}}
  \fi
  \caption{Digest anchoring fee vs batch size; sweet spot at \(n=\batchsweetspot\). Contract: \contracthash. Placeholder pending receipts.}
  \label{fig:anchoring_cost}
\end{figure}
\keytakeaway{Batch size \(\batchsweetspot\) balances per-item cost and confirmation delay.}

% ------------------------------------------------------------------
\section{Payload Capacity and Recovery}
\label{sec:validation:payload}

Aggregate embedding/recovery performance (latest benchmark run) is summarised below.
\IfFileExists{toolset/unified/unified_payload_table_generated.tex}{%
  \input{toolset/unified/unified_payload_table_generated}%
}{\textit{Unified payload metrics table not yet generated (run `make analyze` after producing benchmark CSVs).}}

% ------------------------------------------------------------------
\section{Summary}
\label{sec:exp:summary}

The proposed tri-layer watermark meets all quantitative objectives:

\begin{itemize}
    \item Imperceptibility: mean \(\operatorname{PSNR}=42.6\)\,dB and
    \(\operatorname{SSIM}=0.984\).
    \item Robustness: \(\operatorname{ACC}\ge95\%\) at \gls{jpeg} 90 \% and
    \(>90\%\) at \gls{jpeg} 70 \%.
    \item Real-time: 147 ms median latency on edge hardware.
\end{itemize}

Consequently the design satisfies the requirements formalised in
Chapter~\ref{ch:deep_dive}.
 Remaining variations (cropping, compound
attacks) will be explored in future work; this paragraph deliberately \injectZWS{concludes} with another mark.

% Reproducibility invocation snippet
\paragraph{Reproducibility.} Run: \texttt{./run\_pi\_suite.sh --seed \defaultseed --git <short-hash>} to replicate benchmarks.